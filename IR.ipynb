{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos\n",
    "data = pd.read_csv(\"../Cluster0ReadyToNN.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un diccionario para almacenar los objetos scaler por grupo\n",
    "scalers = {}\n",
    "\n",
    "# Iterar sobre los grupos únicos en Column15\n",
    "for group in data['Column15'].unique():\n",
    "    # Filtrar datos por grupo\n",
    "    group_data = data[data['Column15'] == group]\n",
    "\n",
    "    # Seleccionar las columnas para normalización (las 13 primeras)\n",
    "    features = group_data.iloc[:, :13]\n",
    "\n",
    "    # Normalizar los datos con MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(features)\n",
    "\n",
    "    # Almacenar el scaler en el diccionario\n",
    "    scalers[group] = scaler\n",
    "\n",
    "    # Actualizar el DataFrame con los datos normalizados\n",
    "    data.loc[data['Column15'] == group, 'Column1':'Column13'] = normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column1</th>\n",
       "      <th>Column2</th>\n",
       "      <th>Column3</th>\n",
       "      <th>Column4</th>\n",
       "      <th>Column5</th>\n",
       "      <th>Column6</th>\n",
       "      <th>Column7</th>\n",
       "      <th>Column8</th>\n",
       "      <th>Column9</th>\n",
       "      <th>Column10</th>\n",
       "      <th>Column11</th>\n",
       "      <th>Column12</th>\n",
       "      <th>Column13</th>\n",
       "      <th>Column14</th>\n",
       "      <th>Column15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.317757</td>\n",
       "      <td>0.163551</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2002/3</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.163551</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.210280</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>2002/4</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.210280</td>\n",
       "      <td>0.219626</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>2003/1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.210280</td>\n",
       "      <td>0.219626</td>\n",
       "      <td>0.163551</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>2003/2</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.219626</td>\n",
       "      <td>0.163551</td>\n",
       "      <td>0.214953</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>2003/3</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170581</th>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>2014/4</td>\n",
       "      <td>81579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170582</th>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>2015/1</td>\n",
       "      <td>81579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170583</th>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>2015/2</td>\n",
       "      <td>81579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170584</th>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>2015/3</td>\n",
       "      <td>81579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170585</th>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>2015/4</td>\n",
       "      <td>81579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170586 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Column1   Column2   Column3   Column4   Column5   Column6   Column7  \\\n",
       "0       0.317757  0.163551  1.000000  0.957447  1.000000  0.750000  0.979167   \n",
       "1       0.163551  1.000000  0.210280  1.000000  0.744681  0.979167  0.708333   \n",
       "2       1.000000  0.210280  0.219626  0.744681  0.978723  0.708333  0.979167   \n",
       "3       0.210280  0.219626  0.163551  0.978723  0.702128  0.979167  1.000000   \n",
       "4       0.219626  0.163551  0.214953  0.702128  0.978723  1.000000  0.520833   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "170581  0.017857  0.023810  0.047619  0.023810  0.029762  0.029762  0.029762   \n",
       "170582  0.023810  0.047619  0.023810  0.029762  0.029762  0.029762  0.017857   \n",
       "170583  0.047619  0.023810  0.029762  0.029762  0.029762  0.017857  0.041667   \n",
       "170584  0.023810  0.029762  0.029762  0.029762  0.017857  0.041667  0.023810   \n",
       "170585  0.029762  0.029762  0.029762  0.017857  0.041667  0.023810  0.017857   \n",
       "\n",
       "         Column8   Column9  Column10  Column11  Column12  Column13 Column14  \\\n",
       "0       0.708333  0.979167  1.000000  0.641026  0.897436  1.000000   2002/3   \n",
       "1       0.979167  1.000000  0.520833  0.897436  1.000000  0.871795   2002/4   \n",
       "2       1.000000  0.520833  0.729167  1.000000  0.871795  0.846154   2003/1   \n",
       "3       0.520833  0.729167  0.812500  0.871795  0.846154  0.717949   2003/2   \n",
       "4       0.729167  0.812500  0.708333  0.846154  0.717949  0.846154   2003/3   \n",
       "...          ...       ...       ...       ...       ...       ...      ...   \n",
       "170581  0.017857  0.041667  0.023810  0.017857  0.023810  0.011905   2014/4   \n",
       "170582  0.041667  0.023810  0.017857  0.023810  0.011905  0.023810   2015/1   \n",
       "170583  0.023810  0.017857  0.023810  0.011905  0.023810  0.023810   2015/2   \n",
       "170584  0.017857  0.023810  0.011905  0.023810  0.023810  0.029762   2015/3   \n",
       "170585  0.023810  0.011905  0.023810  0.023810  0.029762  0.011905   2015/4   \n",
       "\n",
       "        Column15  \n",
       "0             23  \n",
       "1             23  \n",
       "2             23  \n",
       "3             23  \n",
       "4             23  \n",
       "...          ...  \n",
       "170581     81579  \n",
       "170582     81579  \n",
       "170583     81579  \n",
       "170584     81579  \n",
       "170585     81579  \n",
       "\n",
       "[170586 rows x 15 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83587 35823 12\n"
     ]
    }
   ],
   "source": [
    "# Ordenar el DataFrame por 'Column 14' de forma ascendente\n",
    "data = data.sort_values(by='Column14')\n",
    "\n",
    "# Dividir los datos en entrenamiento (70%) y temporal (30%)\n",
    "train_temp_data, test_data = train_test_split(data, test_size=0.3, stratify=data['Column15'], random_state=0)\n",
    "#train_temp_data, test_data = train_test_split(data, test_size=0.3, shuffle=False, random_state=0)\n",
    "\n",
    "# Dividir el temporal en entrenamiento (70%) y validación (30%)\n",
    "train_data, validation_data = train_test_split(train_temp_data, test_size=0.3, stratify=train_temp_data['Column15'], random_state=0)\n",
    "#train_data, validation_data = train_test_split(train_temp_data, test_size=0.3, shuffle=False, random_state=0)\n",
    "\n",
    "# Separar características (X) y columna objetivo (y)\n",
    "X_train = train_data.iloc[:, :12]\n",
    "y_train = train_data['Column13']\n",
    "X_val = validation_data.iloc[:, :12]\n",
    "y_val = validation_data['Column13']\n",
    "X_test = test_data.iloc[:, :12]\n",
    "y_test = test_data['Column13']\n",
    "\n",
    "\n",
    "# Reshape de los datos para GRU (número de muestras, número de pasos de tiempo, número de características)\n",
    "n_samples_train, n_features = X_train.shape\n",
    "n_samples_val = X_val.shape[0]\n",
    "n_timesteps = 1\n",
    "X_train = X_train.values.reshape(n_samples_train, n_timesteps, n_features)\n",
    "X_val = X_val.values.reshape(n_samples_val, n_timesteps, n_features)\n",
    "X_test = X_test.values.reshape(X_test.shape[0], n_timesteps, n_features)\n",
    "\n",
    "print(n_samples_train, n_samples_val, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir X_test a formato compatible para modelos convencionales\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], n_features)\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], n_features)\n",
    "X_val_flat = X_val.reshape(X_val.shape[0], n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21012934 0.21012934 0.18295241 ... 0.30165439 0.25132818 0.16743314]\n"
     ]
    }
   ],
   "source": [
    "# 1. Modelo de Isotonic Regression\n",
    "iso_model = IsotonicRegression()\n",
    "iso_model.fit(X_train_flat[:, 0], y_train)  # Requiere 1D para características\n",
    "y_pred = iso_model.predict(X_test_flat[:, 0])\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Valor Real  Predicciones\n",
      "0        0.128378      0.210129\n",
      "1        0.347826      0.210129\n",
      "2        0.051724      0.182952\n",
      "3        0.295400      0.227304\n",
      "4        0.155556      0.200210\n",
      "...           ...           ...\n",
      "51171    0.321429      0.193014\n",
      "51172    0.011936      0.135212\n",
      "51173    0.053030      0.301654\n",
      "51174    0.312155      0.251328\n",
      "51175    0.600000      0.167433\n",
      "\n",
      "[51176 rows x 2 columns]\n",
      "        Column1  Column2  Column3  Column4  Column5  Column6  Column7  \\\n",
      "24905      58.0     66.0     62.0     56.0     77.0     60.0    170.0   \n",
      "155320    101.0     39.0     24.0     28.0     65.0     32.0     28.0   \n",
      "8745       30.0     30.0     29.0     31.0     31.0     31.0     29.0   \n",
      "60462     193.0     59.0     48.0     87.0    186.0     59.0     47.0   \n",
      "4241       35.0     29.0     37.0     31.0     29.0     32.0     32.0   \n",
      "...         ...      ...      ...      ...      ...      ...      ...   \n",
      "132964     37.0     35.0     33.0     35.0     34.0     36.0     21.0   \n",
      "130033      5.0      8.0      7.0     11.0      1.0      7.0      8.0   \n",
      "124375    137.0    120.0     63.0     60.0     54.0     73.0      6.0   \n",
      "50855     185.0     32.0     49.0     67.0     51.0     37.0     56.0   \n",
      "5442       29.0     65.0     44.0     31.0     50.0     69.0     48.0   \n",
      "\n",
      "        Column8  Column9  Column10  Column11  Column12  Column13 Column14  \\\n",
      "24905      69.0     66.0      48.0      53.0  0.216216      41.0   2005/2   \n",
      "155320     45.0     57.0      35.0      27.0  0.186335      59.0   2006/3   \n",
      "8745       29.0     25.0      27.0      26.0  0.155172      23.0   2015/2   \n",
      "60462      52.0    162.0      43.0      36.0  0.053269     150.0   2011/3   \n",
      "4241       23.0     27.0      28.0     137.0  0.096296      34.0   2009/4   \n",
      "...         ...      ...       ...       ...       ...       ...      ...   \n",
      "132964     22.0     44.0      31.0      26.0  0.090909      65.5   2006/3   \n",
      "130033      6.0     12.0       8.0      11.0  0.007958      10.0   2002/4   \n",
      "124375     26.0     32.0      14.0      14.0  0.060606      13.0   2005/4   \n",
      "50855     159.0    109.0      56.0      70.0  0.908840     145.0   2012/4   \n",
      "5442       40.0     55.0      53.0      52.0  0.463636      85.0   2013/1   \n",
      "\n",
      "        Column15  Predicted_Column13  \n",
      "24905       8751           53.099143  \n",
      "155320     56704           36.830824  \n",
      "8745        3059           30.611240  \n",
      "60462      20496          121.876560  \n",
      "4241        1242           40.028309  \n",
      "...          ...                 ...  \n",
      "132964     46569           21.404382  \n",
      "130033     45459          102.949620  \n",
      "124375     43274           45.818379  \n",
      "50855      17695          122.980800  \n",
      "5442        1656           37.417645  \n",
      "\n",
      "[51176 rows x 16 columns]\n",
      "        Column13 Column14  Column15  Predicted_Column13\n",
      "24905       41.0   2005/2      8751           53.099143\n",
      "155320      59.0   2006/3     56704           36.830824\n",
      "8745        23.0   2015/2      3059           30.611240\n",
      "60462      150.0   2011/3     20496          121.876560\n",
      "4241        34.0   2009/4      1242           40.028309\n",
      "...          ...      ...       ...                 ...\n",
      "132964      65.5   2006/3     46569           21.404382\n",
      "130033      10.0   2002/4     45459          102.949620\n",
      "124375      13.0   2005/4     43274           45.818379\n",
      "50855      145.0   2012/4     17695          122.980800\n",
      "5442        85.0   2013/1      1656           37.417645\n",
      "\n",
      "[51176 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame con las predicciones desnormalizadas y los valores reales\n",
    "resultados = pd.DataFrame({'Valor Real': y_test.values.flatten(), 'Predicciones': y_pred.flatten()})\n",
    "print(resultados)\n",
    "\n",
    "# Agregar la columna de predicciones al conjunto de prueba\n",
    "test_data['Predicted_Column13'] = y_pred.flatten()\n",
    "\n",
    "# Crear un DataFrame para almacenar los resultados desnormalizados\n",
    "desnormalized_test_data = test_data.copy()\n",
    "\n",
    "# Desnormalizar 'Column1' a 'Column13' y 'Predicted_Column13' según la normalización por grupos\n",
    "for group, scalerY in scalers.items():\n",
    "    # Filtrar el conjunto de prueba correspondiente al grupo\n",
    "    group_test_data = test_data[test_data['Column15'] == group]\n",
    "\n",
    "    # Seleccionar las columnas normalizadas para desnormalizar\n",
    "    normalized_features = group_test_data[['Column1', 'Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column7', 'Column8', 'Column9', 'Column10', 'Column11', 'Column13', 'Predicted_Column13']]\n",
    "\n",
    "    # Desnormalizar los datos utilizando el objeto scalerY correspondiente\n",
    "    original_data = scalerY.inverse_transform(normalized_features)\n",
    "\n",
    "    # Crear un DataFrame temporal para almacenar los datos desnormalizados\n",
    "    temp_df = pd.DataFrame(original_data, columns=['Column1', 'Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column7', 'Column8', 'Column9', 'Column10', 'Column11', 'Column13', 'Predicted_Column13'])\n",
    "\n",
    "    # Actualizar el DataFrame desnormalizado con los datos desnormalizados\n",
    "    desnormalized_test_data.loc[desnormalized_test_data['Column15'] == group, ['Column1', 'Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column7', 'Column8', 'Column9', 'Column10', 'Column11', 'Column13', 'Predicted_Column13']] = temp_df.values\n",
    "\n",
    "# Imprimir el conjunto de prueba después de la desnormalización\n",
    "#print(desnormalized_test_data)\n",
    "\n",
    "# Eliminar todas las columnas excepto las últimas cuatro\n",
    "resultados = desnormalized_test_data.iloc[:, -4:]\n",
    "\n",
    "# Imprimir el conjunto de prueba después de la eliminación de columnas\n",
    "#print(resultados)\n",
    "\n",
    "# Guardar el DataFrame resultados en un archivo CSV\n",
    "resultados.to_csv('PR_norm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test: 24905      41.0\n",
      "155320     59.0\n",
      "8745       23.0\n",
      "60462     150.0\n",
      "4241       34.0\n",
      "          ...  \n",
      "132964     65.5\n",
      "130033     10.0\n",
      "124375     13.0\n",
      "50855     145.0\n",
      "5442       85.0\n",
      "Name: Column13, Length: 51176, dtype: float64\n",
      "y_pred: 24905      53.099143\n",
      "155320     36.830824\n",
      "8745       30.611240\n",
      "60462     121.876560\n",
      "4241       40.028309\n",
      "             ...    \n",
      "132964     21.404382\n",
      "130033    102.949620\n",
      "124375     45.818379\n",
      "50855     122.980800\n",
      "5442       37.417645\n",
      "Name: Predicted_Column13, Length: 51176, dtype: float64\n",
      "        Column13 Column14  Column15  Predicted_Column13\n",
      "24905       41.0   2005/2      8751           53.099143\n",
      "155320      59.0   2006/3     56704           36.830824\n",
      "8745        23.0   2015/2      3059           30.611240\n",
      "60462      150.0   2011/3     20496          121.876560\n",
      "4241        34.0   2009/4      1242           40.028309\n",
      "...          ...      ...       ...                 ...\n",
      "132964      65.5   2006/3     46569           21.404382\n",
      "130033      10.0   2002/4     45459          102.949620\n",
      "124375      13.0   2005/4     43274           45.818379\n",
      "50855      145.0   2012/4     17695          122.980800\n",
      "5442        85.0   2013/1      1656           37.417645\n",
      "\n",
      "[51176 rows x 4 columns]\n",
      "Valores NaN en y_test: 0\n",
      "Valores NaN en y_pred: 934\n",
      "Hay valores NaN en los datos de prueba o predicción. Procediendo a eliminarlos.\n",
      "RMSE en el conjunto de prueba: 54.52445453752387\n",
      "MAE en el conjunto de prueba: 25.162683085763863\n",
      "Mean absolute percentage error (MAPE): 1.326081\n"
     ]
    }
   ],
   "source": [
    "# Obtener y_test de la primera columna de resultados\n",
    "y_test = resultados['Column13']\n",
    "\n",
    "# Obtener y_pred de la última columna del conjunto de prueba después de la desnormalización\n",
    "y_pred = desnormalized_test_data['Predicted_Column13']\n",
    "\n",
    "# Imprimir y_test_norm y y_pred\n",
    "print(\"y_test:\", y_test)\n",
    "print(\"y_pred:\", y_pred)\n",
    "\n",
    "# Imprimir el DataFrame\n",
    "print(resultados)\n",
    "# Guardar el DataFrame resultados en un archivo CSV\n",
    "#resultados.to_csv('mejorResultadosWaterx1.csv', index=False)\n",
    "\n",
    "# Contar valores NaN en y_test y y_pred\n",
    "nan_y_test = y_test.isna().sum()\n",
    "nan_y_pred = y_pred.isna().sum()\n",
    "\n",
    "print(f'Valores NaN en y_test: {nan_y_test}')\n",
    "print(f'Valores NaN en y_pred: {nan_y_pred}')\n",
    "\n",
    "\n",
    "# Comprobar si hay valores NaN en y_test o y_pred\n",
    "if y_test.isna().any() or y_pred.isna().any():\n",
    "    print(\"Hay valores NaN en los datos de prueba o predicción. Procediendo a eliminarlos.\")\n",
    "    # Eliminar filas con NaN\n",
    "    valid_data = ~y_test.isna() & ~y_pred.isna()\n",
    "    y_test = y_test[valid_data]\n",
    "    y_pred = y_pred[valid_data]\n",
    "\n",
    "# Calcular RMSE con datos desnormalizados\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'RMSE en el conjunto de prueba: {rmse}')\n",
    "\n",
    "# Calcular MAE con datos desnormalizados\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'MAE en el conjunto de prueba: {mae}')\n",
    "\n",
    "\n",
    "#Calcular MAPE con datos desnormalizados\n",
    "print(\"Mean absolute percentage error (MAPE): %f\" % mean_absolute_percentage_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Año       Column13  Predicted_Column13  MAPE_fila\n",
      "0   2002  104265.678499        87761.365978  15.829094\n",
      "1   2003  177491.021691       173116.943718   2.464394\n",
      "2   2004  164849.403889       168563.122959   2.252795\n",
      "3   2005  159059.300841       166085.306417   4.417224\n",
      "4   2006  131381.875214       162911.044169  23.998112\n",
      "5   2007  131223.654627       165114.520703  25.826796\n",
      "6   2008  122996.287059       164395.955706  33.659283\n",
      "7   2009  128977.774184       163019.956804  26.393836\n",
      "8   2010  117483.995234       150931.913385  28.470191\n",
      "9   2011  118736.260722       158395.644093  33.401240\n",
      "10  2012  122697.561296       165194.469745  34.635496\n",
      "11  2013  109818.900063       156973.770619  42.938757\n",
      "12  2014  104937.636563       153244.277933  46.033666\n",
      "13  2015  109243.945117       153244.508968  40.277348\n",
      "Media de MAPE (por fila): 25.76%\n"
     ]
    }
   ],
   "source": [
    "# Convertir la columna 'Column14' para extraer el año\n",
    "resultados['Año'] = resultados['Column14'].str.split('/').str[0].astype(int)\n",
    "\n",
    "# Agrupar por año y calcular la suma de reales y predicciones\n",
    "suma_anual = resultados.groupby('Año').agg({\n",
    "    'Column13': 'sum',  # Suma de valores reales\n",
    "    'Predicted_Column13': 'sum'  # Suma de predicciones\n",
    "}).reset_index()\n",
    "\n",
    "# Agregar una columna de MAPE por fila\n",
    "suma_anual['MAPE_fila'] = (\n",
    "    (abs(suma_anual['Column13'] - suma_anual['Predicted_Column13']) / suma_anual['Column13']) * 100\n",
    ")\n",
    "\n",
    "# Calcular la media de la columna MAPE\n",
    "media_mape = suma_anual['MAPE_fila'].mean()\n",
    "\n",
    "# Mostrar el DataFrame actualizado y la media\n",
    "print(suma_anual)\n",
    "print(f\"Media de MAPE (por fila): {media_mape:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

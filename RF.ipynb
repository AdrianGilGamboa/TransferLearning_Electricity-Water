{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos\n",
    "data = pd.read_csv(\"../Cluster0ReadyToNN.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un diccionario para almacenar los objetos scaler por grupo\n",
    "scalers = {}\n",
    "\n",
    "# Iterar sobre los grupos únicos en Column15\n",
    "for group in data['Column15'].unique():\n",
    "    # Filtrar datos por grupo\n",
    "    group_data = data[data['Column15'] == group]\n",
    "\n",
    "    # Seleccionar las columnas para normalización (las 13 primeras)\n",
    "    features = group_data.iloc[:, :13]\n",
    "\n",
    "    # Normalizar los datos con MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(features)\n",
    "\n",
    "    # Almacenar el scaler en el diccionario\n",
    "    scalers[group] = scaler\n",
    "\n",
    "    # Actualizar el DataFrame con los datos normalizados\n",
    "    data.loc[data['Column15'] == group, 'Column1':'Column13'] = normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column1</th>\n",
       "      <th>Column2</th>\n",
       "      <th>Column3</th>\n",
       "      <th>Column4</th>\n",
       "      <th>Column5</th>\n",
       "      <th>Column6</th>\n",
       "      <th>Column7</th>\n",
       "      <th>Column8</th>\n",
       "      <th>Column9</th>\n",
       "      <th>Column10</th>\n",
       "      <th>Column11</th>\n",
       "      <th>Column12</th>\n",
       "      <th>Column13</th>\n",
       "      <th>Column14</th>\n",
       "      <th>Column15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.317757</td>\n",
       "      <td>0.163551</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2002/3</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.163551</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.210280</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>2002/4</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.210280</td>\n",
       "      <td>0.219626</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>2003/1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.210280</td>\n",
       "      <td>0.219626</td>\n",
       "      <td>0.163551</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>2003/2</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.219626</td>\n",
       "      <td>0.163551</td>\n",
       "      <td>0.214953</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>2003/3</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170581</th>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>2014/4</td>\n",
       "      <td>81579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170582</th>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>2015/1</td>\n",
       "      <td>81579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170583</th>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>2015/2</td>\n",
       "      <td>81579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170584</th>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>2015/3</td>\n",
       "      <td>81579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170585</th>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>2015/4</td>\n",
       "      <td>81579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170586 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Column1   Column2   Column3  ...  Column13  Column14  Column15\n",
       "0       0.317757  0.163551  1.000000  ...  1.000000    2002/3        23\n",
       "1       0.163551  1.000000  0.210280  ...  0.871795    2002/4        23\n",
       "2       1.000000  0.210280  0.219626  ...  0.846154    2003/1        23\n",
       "3       0.210280  0.219626  0.163551  ...  0.717949    2003/2        23\n",
       "4       0.219626  0.163551  0.214953  ...  0.846154    2003/3        23\n",
       "...          ...       ...       ...  ...       ...       ...       ...\n",
       "170581  0.017857  0.023810  0.047619  ...  0.011905    2014/4     81579\n",
       "170582  0.023810  0.047619  0.023810  ...  0.023810    2015/1     81579\n",
       "170583  0.047619  0.023810  0.029762  ...  0.023810    2015/2     81579\n",
       "170584  0.023810  0.029762  0.029762  ...  0.029762    2015/3     81579\n",
       "170585  0.029762  0.029762  0.029762  ...  0.011905    2015/4     81579\n",
       "\n",
       "[170586 rows x 15 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83587 35823 12\n"
     ]
    }
   ],
   "source": [
    "# Ordenar el DataFrame por 'Column 14' de forma ascendente\n",
    "data = data.sort_values(by='Column14')\n",
    "\n",
    "# Dividir los datos en entrenamiento (70%) y temporal (30%)\n",
    "train_temp_data, test_data = train_test_split(data, test_size=0.3, stratify=data['Column15'], random_state=0)\n",
    "#train_temp_data, test_data = train_test_split(data, test_size=0.3, shuffle=False, random_state=0)\n",
    "\n",
    "# Dividir el temporal en entrenamiento (70%) y validación (30%)\n",
    "train_data, validation_data = train_test_split(train_temp_data, test_size=0.3, stratify=train_temp_data['Column15'], random_state=0)\n",
    "#train_data, validation_data = train_test_split(train_temp_data, test_size=0.3, shuffle=False, random_state=0)\n",
    "\n",
    "# Separar características (X) y columna objetivo (y)\n",
    "X_train = train_data.iloc[:, :12]\n",
    "y_train = train_data['Column13']\n",
    "X_val = validation_data.iloc[:, :12]\n",
    "y_val = validation_data['Column13']\n",
    "X_test = test_data.iloc[:, :12]\n",
    "y_test = test_data['Column13']\n",
    "\n",
    "\n",
    "# Reshape de los datos para GRU (número de muestras, número de pasos de tiempo, número de características)\n",
    "n_samples_train, n_features = X_train.shape\n",
    "n_samples_val = X_val.shape[0]\n",
    "n_timesteps = 1\n",
    "X_train = X_train.values.reshape(n_samples_train, n_timesteps, n_features)\n",
    "X_val = X_val.values.reshape(n_samples_val, n_timesteps, n_features)\n",
    "X_test = X_test.values.reshape(X_test.shape[0], n_timesteps, n_features)\n",
    "\n",
    "print(n_samples_train, n_samples_val, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir X_test a formato compatible para modelos convencionales\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], n_features)\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], n_features)\n",
    "X_val_flat = X_val.reshape(X_val.shape[0], n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Modelo de Random Forest (cambiar a 100)\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "rf_model.fit(X_train_flat, y_train)\n",
    "y_pred = rf_model.predict(X_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Valor Real  Predicciones\n",
      "0        0.128378      0.286216\n",
      "1        0.347826      0.302754\n",
      "2        0.051724      0.136529\n",
      "3        0.295400      0.259615\n",
      "4        0.155556      0.140714\n",
      "...           ...           ...\n",
      "51171    0.321429      0.141612\n",
      "51172    0.011936      0.059358\n",
      "51173    0.053030      0.242313\n",
      "51174    0.312155      0.347450\n",
      "51175    0.600000      0.544825\n",
      "\n",
      "[51176 rows x 2 columns]\n",
      "        Column1  Column2  Column3  ...  Column14  Column15  Predicted_Column13\n",
      "24905      58.0     66.0     62.0  ...    2005/2      8751           64.359990\n",
      "155320    101.0     39.0     24.0  ...    2006/3     56704           51.743373\n",
      "8745       30.0     30.0     29.0  ...    2015/2      3059           27.918659\n",
      "60462     193.0     59.0     48.0  ...    2011/3     20496          135.220982\n",
      "4241       35.0     29.0     37.0  ...    2009/4      1242           31.996362\n",
      "...         ...      ...      ...  ...       ...       ...                 ...\n",
      "132964     37.0     35.0     33.0  ...    2006/3     46569           19.965144\n",
      "130033      5.0      8.0      7.0  ...    2002/4     45459           45.755621\n",
      "124375    137.0    120.0     63.0  ...    2005/4     43274           37.985305\n",
      "50855     185.0     32.0     49.0  ...    2012/4     17695          157.776929\n",
      "5442       29.0     65.0     44.0  ...    2013/1      1656           78.930704\n",
      "\n",
      "[51176 rows x 16 columns]\n",
      "        Column13 Column14  Column15  Predicted_Column13\n",
      "24905       41.0   2005/2      8751           64.359990\n",
      "155320      59.0   2006/3     56704           51.743373\n",
      "8745        23.0   2015/2      3059           27.918659\n",
      "60462      150.0   2011/3     20496          135.220982\n",
      "4241        34.0   2009/4      1242           31.996362\n",
      "...          ...      ...       ...                 ...\n",
      "132964      65.5   2006/3     46569           19.965144\n",
      "130033      10.0   2002/4     45459           45.755621\n",
      "124375      13.0   2005/4     43274           37.985305\n",
      "50855      145.0   2012/4     17695          157.776929\n",
      "5442        85.0   2013/1      1656           78.930704\n",
      "\n",
      "[51176 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame con las predicciones desnormalizadas y los valores reales\n",
    "resultados = pd.DataFrame({'Valor Real': y_test.values.flatten(), 'Predicciones': y_pred.flatten()})\n",
    "print(resultados)\n",
    "\n",
    "# Agregar la columna de predicciones al conjunto de prueba\n",
    "test_data['Predicted_Column13'] = y_pred.flatten()\n",
    "\n",
    "# Crear un DataFrame para almacenar los resultados desnormalizados\n",
    "desnormalized_test_data = test_data.copy()\n",
    "\n",
    "# Desnormalizar 'Column1' a 'Column13' y 'Predicted_Column13' según la normalización por grupos\n",
    "for group, scalerY in scalers.items():\n",
    "    # Filtrar el conjunto de prueba correspondiente al grupo\n",
    "    group_test_data = test_data[test_data['Column15'] == group]\n",
    "\n",
    "    # Seleccionar las columnas normalizadas para desnormalizar\n",
    "    normalized_features = group_test_data[['Column1', 'Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column7', 'Column8', 'Column9', 'Column10', 'Column11', 'Column13', 'Predicted_Column13']]\n",
    "\n",
    "    # Desnormalizar los datos utilizando el objeto scalerY correspondiente\n",
    "    original_data = scalerY.inverse_transform(normalized_features)\n",
    "\n",
    "    # Crear un DataFrame temporal para almacenar los datos desnormalizados\n",
    "    temp_df = pd.DataFrame(original_data, columns=['Column1', 'Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column7', 'Column8', 'Column9', 'Column10', 'Column11', 'Column13', 'Predicted_Column13'])\n",
    "\n",
    "    # Actualizar el DataFrame desnormalizado con los datos desnormalizados\n",
    "    desnormalized_test_data.loc[desnormalized_test_data['Column15'] == group, ['Column1', 'Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column7', 'Column8', 'Column9', 'Column10', 'Column11', 'Column13', 'Predicted_Column13']] = temp_df.values\n",
    "\n",
    "# Imprimir el conjunto de prueba después de la desnormalización\n",
    "print(desnormalized_test_data)\n",
    "\n",
    "# Eliminar todas las columnas excepto las últimas cuatro\n",
    "resultados = desnormalized_test_data.iloc[:, -4:]\n",
    "\n",
    "# Imprimir el conjunto de prueba después de la eliminación de columnas\n",
    "print(resultados)\n",
    "\n",
    "# Guardar el DataFrame resultados en un archivo CSV\n",
    "resultados.to_csv('RF_norm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test: 24905      41.0\n",
      "155320     59.0\n",
      "8745       23.0\n",
      "60462     150.0\n",
      "4241       34.0\n",
      "          ...  \n",
      "132964     65.5\n",
      "130033     10.0\n",
      "124375     13.0\n",
      "50855     145.0\n",
      "5442       85.0\n",
      "Name: Column13, Length: 51176, dtype: float64\n",
      "y_pred: 24905      64.359990\n",
      "155320     51.743373\n",
      "8745       27.918659\n",
      "60462     135.220982\n",
      "4241       31.996362\n",
      "             ...    \n",
      "132964     19.965144\n",
      "130033     45.755621\n",
      "124375     37.985305\n",
      "50855     157.776929\n",
      "5442       78.930704\n",
      "Name: Predicted_Column13, Length: 51176, dtype: float64\n",
      "        Column13 Column14  Column15  Predicted_Column13   Año\n",
      "24905       41.0   2005/2      8751           64.359990  2005\n",
      "155320      59.0   2006/3     56704           51.743373  2006\n",
      "8745        23.0   2015/2      3059           27.918659  2015\n",
      "60462      150.0   2011/3     20496          135.220982  2011\n",
      "4241        34.0   2009/4      1242           31.996362  2009\n",
      "...          ...      ...       ...                 ...   ...\n",
      "132964      65.5   2006/3     46569           19.965144  2006\n",
      "130033      10.0   2002/4     45459           45.755621  2002\n",
      "124375      13.0   2005/4     43274           37.985305  2005\n",
      "50855      145.0   2012/4     17695          157.776929  2012\n",
      "5442        85.0   2013/1      1656           78.930704  2013\n",
      "\n",
      "[51176 rows x 5 columns]\n",
      "RMSE en el conjunto de prueba: 44.41325898445496\n",
      "MAE en el conjunto de prueba: 15.643574214234679\n",
      "Mean absolute percentage error (MAPE): 0.732992\n"
     ]
    }
   ],
   "source": [
    "# Obtener y_test de la primera columna de resultados\n",
    "y_test = resultados['Column13']\n",
    "\n",
    "# Obtener y_pred de la última columna del conjunto de prueba después de la desnormalización\n",
    "y_pred = desnormalized_test_data['Predicted_Column13']\n",
    "\n",
    "# Imprimir y_test_norm y y_pred\n",
    "print(\"y_test:\", y_test)\n",
    "print(\"y_pred:\", y_pred)\n",
    "\n",
    "# Imprimir el DataFrame\n",
    "print(resultados)\n",
    "\n",
    "# Calcular RMSE con datos desnormalizados\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'RMSE en el conjunto de prueba: {rmse}')\n",
    "\n",
    "# Calcular MAE con datos desnormalizados\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'MAE en el conjunto de prueba: {mae}')\n",
    "\n",
    "\n",
    "#Calcular MAPE con datos desnormalizados\n",
    "print(\"Mean absolute percentage error (MAPE): %f\" % mean_absolute_percentage_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Año       Column13  Predicted_Column13  MAPE_fila\n",
      "0   2002  104265.678499       101945.325361   2.225424\n",
      "1   2003  177491.021691       169555.493758   4.470946\n",
      "2   2004  164849.403889       167229.529217   1.443818\n",
      "3   2005  159059.300841       161731.790701   1.680185\n",
      "4   2006  131381.875214       147401.301614  12.193026\n",
      "5   2007  131223.654627       141705.449997   7.987733\n",
      "6   2008  122996.287059       139852.845312  13.704933\n",
      "7   2009  128977.774184       134388.332356   4.194954\n",
      "8   2010  117483.995234       129267.617701  10.029981\n",
      "9   2011  118736.260722       133501.598650  12.435408\n",
      "10  2012  122697.561296       138659.676133  13.009317\n",
      "11  2013  109818.900063       128769.987875  17.256672\n",
      "12  2014  104937.636563       125031.158762  19.148061\n",
      "13  2015  109243.945117       126577.969801  15.867264\n",
      "Media de MAPE (por fila): 9.69%\n"
     ]
    }
   ],
   "source": [
    "# Convertir la columna 'Column14' para extraer el año\n",
    "resultados['Año'] = resultados['Column14'].str.split('/').str[0].astype(int)\n",
    "\n",
    "# Agrupar por año y calcular la suma de reales y predicciones\n",
    "suma_anual = resultados.groupby('Año').agg({\n",
    "    'Column13': 'sum',  # Suma de valores reales\n",
    "    'Predicted_Column13': 'sum'  # Suma de predicciones\n",
    "}).reset_index()\n",
    "\n",
    "# Agregar una columna de MAPE por fila\n",
    "suma_anual['MAPE_fila'] = (\n",
    "    (abs(suma_anual['Column13'] - suma_anual['Predicted_Column13']) / suma_anual['Column13']) * 100\n",
    ")\n",
    "\n",
    "# Calcular la media de la columna MAPE\n",
    "media_mape = suma_anual['MAPE_fila'].mean()\n",
    "\n",
    "# Mostrar el DataFrame actualizado y la media\n",
    "print(suma_anual)\n",
    "print(f\"Media de MAPE (por fila): {media_mape:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
